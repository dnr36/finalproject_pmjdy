---
title: "Data Science for Public Policy"
subtitle: "Final Project: Predicting the PMJDY Takeup Rate in India"
author: "Meenakshi Alagusundaram - ma2309, Sanya Bahal - sb2063, Diana Rivas - dnr36, Sona Sarin - srs368"
execute:
  warning: false
format:
  html:
    embed-resources: true
---

## Supervised Machine Learning Models

## Loading Libraries

```{r}

library(tidyverse)
library(haven)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(readr)  
library(ranger)
library(sf)
library(rgeoboundaries)
library(ggthemes)
library(rlang)
library(expss)
library(patchwork)
library(rsample)
library(tidyclust)
library(factoextra)
library(broom)
library(yardstick)
library(vip)
library(parsnip)
library(recipes)
library(themis)


```

## Preparing data 

```{r}
pmjdy <- read_dta("PMJDYmaster.dta")%>% 
  filter(!year == 2021) %>% # There is no pmjdy take up data for this year
  select(!partyinpower_dummy) # Unnecessary variable

# Editing latitude and longitude data due to Punjab, Chandigarh and Haryana having the same coordinates since they have the same capitals. 
pmjdy <- pmjdy %>%
  mutate(
    Latitude = case_when(
      Latitude == 18.9 ~ 19.8,
      Latitude == 17.5 ~ 15.9,
      TRUE ~ Latitude  
    ),
    Longitude = case_when(
      Longitude == 72.8 ~ 75.7, 
      Longitude == 78.6 ~ 79.7,
      TRUE ~ Longitude  
    )
  )

# Changing all variable names to lower case
names(pmjdy) <- tolower(names(pmjdy))

# Editing variables so that they can be used later in the geospatial analysis titles 
pmjdy = apply_labels(pmjdy, 
                     pmjdy = "PMJDY Uptake",
                     healthindex = "Health Index", 
                     incomeindex = "Income Index",
                     educationindex = "Education Index",
                     partyinpower = "Party in Power",
                     employment = "Employment",
                     pop = "Population"
                     )
```

### Set Up for Models and EDA through data visualization

```{r}

# Employment vs Literacy for state Delhi 
# Splitting data into training and testing data 
set.seed(2588596)

pmjdy_split <- initial_split(data = pmjdy, prop = 0.8)

# Creating the training and testing data
df_train <- training(x = pmjdy_split)
df_test  <- testing(x = pmjdy_split)

# Cross validation folds, tried to stratify by state but the data is too small 
folds <- vfold_cv(df_train, v = 10)


delhi_data <- df_train[df_train$state == "Delhi", ]

# Create the plot
eda1 <- ggplot(delhi_data, aes(x = educationindex, y = employment)) +
  geom_point() +  # Adds the dot plot
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  
  labs(x = "Literacy Rate", y = "Employment Rate", title = "Employment vs Literacy in Delhi") +
  theme_minimal()  

# employment vs literacy for all states for the year 2019 

data_2019 <- df_train[df_train$year == 2019, ]

# Create the plot for all states
eda2 <- ggplot(data_2019, aes(x = educationindex, y = employment, color = state)) +
  geom_point() +  
  geom_smooth(method = "lm", se = FALSE, color = "black") +  
  labs(x = "Literacy Rate", y = "Employment Rate", title = "Employment vs Literacy Across States for 2019") +
  theme_minimal() +  # Uses a minimal theme for the plot
  theme(legend.position = "bottom") 

# pmjdy beneficiary rate over 2016 - 2020 
# Calculating the mean of 'pmjdy' for each year
annual_means <- df_train %>%
  group_by(year) %>%
  summarize(mean_pmjdy = mean(pmjdy, na.rm = TRUE))  

# Plotting the annual mean of 'pmjdy'
eda3 <- ggplot(annual_means, aes(x = year, y = mean_pmjdy)) +
  geom_line(color = "blue", size = 1) +  # Adds a blue line connecting the mean values
  geom_point(color = "red", size = 3) +  # Adds red points for each yearly mean
  labs(x = "Year", y = "PMJDY", title = "Beneficiary rate of PMJDY from 2016 to 2020") +
  theme_minimal() +
  theme(legend.position = "none")  # No need for a legend  

eda1

eda2

eda3

```

The graph titled "Employment vs Literacy in Delhi" shows a scatter plot of employment rates plotted against literacy rates in Delhi, with a trend line indicating a positive correlation between the two variables. The data points suggest that as literacy rates increase, employment rates also tend to increase, though the relationship is not very strong given the narrow range of literacy rates and the presence of outliers. The trend line, despite some dispersion in data points, generally supports the idea that higher literacy could be associated with higher employment levels in the region.\
\
The graph titled "Employment vs Literacy Across States for 2019" shows the relationship between literacy rates and employment rates across various Indian states. Each colored dot represents a different state, plotted by its literacy rate on the horizontal axis and employment rate on the vertical axis. The overall trend, as indicated by the black trend line, suggests a negative correlation between literacy rates and employment rates. This means that states with higher literacy rates tend to have slightly lower employment rates, although the relationship does not appear very strong, given the spread of the data points across the plot.\
\
The graph titled "Beneficiary rate of PMJDY from 2016 - 2020" shows the beneficiary rate of the PMJDY program from 2016 to 2020. The rate began at approximately 20 in 2016, increased in 2017, then significantly dropped in 2018, before rising sharply in 2019 and continuing to increase in 2020. This trend suggests fluctuations in the program's uptake over these years, which might be influenced by various factors such as policy changes, economic conditions, or program awareness campaigns. The steady rise after the drop in 2018 indicates recovery and potentially successful interventions to boost enrollment.

## Models

```{r}


# Creating a recipe 

pmjdy_rec <- recipe(pmjdy ~ state + year + incomeindex + educationindex, df_train)%>%
  step_dummy(state) %>%
  step_normalize(all_numeric(), -all_outcomes())

```

### Decision tree model

```{r}
# Decision tree specification 
dt_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("regression")

# Decision tree workflow 
dt_wf <- workflow() %>%
  add_recipe(pmjdy_rec) %>%
  add_model(dt_spec)

# Fitting decision tree
dt_fit_rs <- dt_wf %>%
  fit_resamples(resamples = folds,
                control = control_resamples(save_pred = TRUE),
                metrics = metric_set(rmse))

# Decision tree metrics
dt_metrics <- collect_metrics(dt_fit_rs, summarize = FALSE)


# Calculating mean rsme
tree_mean_rmse <- dt_metrics %>%
  filter(.metric == "rmse") %>%
  pull(.estimate) %>%
  mean()

```

## Lasso model with hyper parameter tuning

```{r}
# Lasso specification

lasso_mod <- linear_reg(penalty = tune(), mixture = tune()) %>%
    set_engine("glmnet") %>%
  set_mode("regression")

# Workflow and tuning grid 

lasso_mod_wf <- workflow() %>%
    add_model(spec = lasso_mod) %>%
    add_recipe(recipe = pmjdy_rec)
    
    # creating tuning grid 
    grid <- grid_regular(penalty(), mixture(), levels = 10)

# Fitting model 
    
lasso_fit_rs <- lasso_mod_wf %>%
    tune_grid(resamples = folds, 
              grid = grid, 
              control = control_grid(save_pred = TRUE),
              metrics = metric_set(rmse))   

# Calculating metrics 

lasso_metrics <- lasso_fit_rs %>%
    collect_metrics(summarize = FALSE)

# Calculating mean RMSE for Lasso

lasso_mean_rmse <- lasso_metrics %>%
  filter(.metric == "rmse") %>%
  pull(.estimate) %>%
  mean()
```

### Random forest model

```{r}
# Define the model specification
rf_spec <- rand_forest(trees = 1000) %>%
  set_mode("regression") %>%
  set_engine("ranger")

# Set up the workflow
rf_wf <- workflow() %>%
  add_recipe(pmjdy_rec) %>%
  add_model(rf_spec)

# Fitting model 
rf_fit_rs <- rf_wf %>%
    fit_resamples(resamples = folds, 
                  control = control_resamples(save_pred = TRUE),
                metrics = metric_set(rmse))

# Calculating metrics 
rf_metrics <- rf_fit_rs %>%
    collect_metrics(summarize = FALSE)

# Mean rmse 
rf_mean_rmse <- rf_metrics %>%
  filter(.metric == "rmse") %>%
  pull(.estimate) %>%
  mean()

```

### Visualizing the models

```{r}

# Visualizing all models

ggplot() +
geom_point(data = lasso_metrics %>% filter(.metric == "rmse"), aes(x= id, y =.estimate, color = "Lasso")) +
geom_point(data = dt_metrics %>% filter(.metric == "rmse"), aes(x= id, y =.estimate, color = "Tree")) +
geom_point(data = rf_metrics %>% filter(.metric == "rmse"), aes(x= id, y =.estimate, color = "KNN")) +
labs( title = "RMSE by model and by fold", color = "Model", x = "Fold", y = "Estimate" )


```

### Estimation

```{r}
# Selecting best models, however from the calculation of the mean RMSE above, we know that lasso leads to the lowest root mean square error 

best_lasso <- lasso_fit_rs %>%
  select_best(metric = "rmse") 

final_lasso <- finalize_workflow(
  lasso_mod_wf,
  best_lasso) %>%
    fit(data = df_train)

# Making predictions on the testing data
predictions <- 
    bind_cols(
        df_test, 
        predict(object = final_lasso, new_data = df_test)
    )

# Printing results
predictions %>%
  select(pmjdy, .pred) %>%
  print()

# Removing label from the "truth" variable, pmjdy, so that the model can be evaluated
predictions = unlab(predictions)

# Evaluating model 
 predictions %>%
  metrics(truth = pmjdy, estimate = .pred)
```

The best machine learning model among the three different options tried was LASSO. This model used hyper parameter tuning to select the best combination and strength of the indicators. The best LASSO model created above has a high R squared with somewhat low RMSE compared to the other models. This means it can be an adequate model to predict the take up of the PMJDY program. We can see that the predictions are a few percentage points off, so although the prediction is approximately accurate, it is important to note that the predictions are not very precise.

For the context of this program, this model can be used to predict the take up of the program in a certain place. The model would be a helpful tool because it can approximately predict if there will be a high enough take up rate that will make the investment/government expenditure worth it.
